{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a9351",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b95852",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c953cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0295977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(H, N, plotPath):\n",
    "\t# construct a plot that plots and saves the training history\n",
    "\tplt.style.use(\"ggplot\")\n",
    "\tplt.figure()\n",
    "\tplt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "\tplt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "\tplt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "\tplt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "\tplt.title(\"Training Loss and Accuracy\")\n",
    "\tplt.xlabel(\"Epoch #\")\n",
    "\tplt.ylabel(\"Loss/Accuracy\")\n",
    "\tplt.legend(loc=\"lower left\")\n",
    "\tplt.savefig(plotPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48868801",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = 'images_dataset3'\n",
    "TRAIN = 'train'\n",
    "TEST = 'test'\n",
    "VAL = 'validation'\n",
    "BATCH_SIZE = 15\n",
    "TRAIN_PATH = os.path.join(BASE_PATH,TRAIN)\n",
    "VAL_PATH = os.path.join(BASE_PATH, VAL)\n",
    "TEST_PATH = os.path.join(BASE_PATH, TEST)\n",
    "# determine the total number of image paths in training, validation,\n",
    "# and testing directories\n",
    "totalTrain = len(list(paths.list_images(trainPath)))\n",
    "totalVal = len(list(paths.list_images(valPath)))\n",
    "totalTest = len(list(paths.list_images(testPath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1b90f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAug = ImageDataGenerator(\n",
    "\trotation_range=25,\n",
    "\tzoom_range=0.1,\n",
    "\twidth_shift_range=0.1,\n",
    "\theight_shift_range=0.1,\n",
    "\tshear_range=0.2,\n",
    "\thorizontal_flip=True,\n",
    "\tfill_mode=\"nearest\")\n",
    "# initialize the validation/testing data augmentation object (which\n",
    "# we'll be adding mean subtraction to)\n",
    "valAug = ImageDataGenerator()\n",
    "# define the ImageNet mean subtraction (in RGB order) and set the\n",
    "# the mean subtraction value for each of the data augmentation\n",
    "# objects\n",
    "mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n",
    "trainAug.mean = mean\n",
    "valAug.mean = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ef3f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the training generator\n",
    "trainGen = trainAug.flow_from_directory(\n",
    "\tTRAIN_PATH,\n",
    "\tclass_mode=\"categorical\",\n",
    "\ttarget_size=(224, 224),\n",
    "\tcolor_mode=\"rgb\",\n",
    "\tshuffle=True,\n",
    "\tbatch_size=BATCH_SIZE)\n",
    "# initialize the validation generator\n",
    "valGen = valAug.flow_from_directory(\n",
    "\tVAL_PATH,\n",
    "\tclass_mode=\"categorical\",\n",
    "\ttarget_size=(224, 224),\n",
    "\tcolor_mode=\"rgb\",\n",
    "\tshuffle=False,\n",
    "\tbatch_size=BATCH_SIZE)\n",
    "# initialize the testing generator\n",
    "testGen = valAug.flow_from_directory(\n",
    "\tTEST_PATH,\n",
    "\tclass_mode=\"categorical\",\n",
    "\ttarget_size=(224, 224),\n",
    "\tcolor_mode=\"rgb\",\n",
    "\tshuffle=False,\n",
    "\tbatch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745d0f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ResNet-50 network, ensuring the head FC layer sets are left\n",
    "# off\n",
    "\n",
    "print(\"[INFO] preparing model...\")\n",
    "baseModel = ResNet50(weights=\"imagenet\", include_top=False,\n",
    "\tinput_tensor=Input(shape=(224, 224, 3)))\n",
    "# construct the head of the model that will be placed on top of the\n",
    "# the base model\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(256, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(5, activation=\"softmax\")(headModel)\n",
    "# place the head FC model on top of the base model (this will become\n",
    "# the actual model we will train)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "# loop over all layers in the base model and freeze them so they will\n",
    "# *not* be updated during the training process\n",
    "for layer in baseModel.layers:\n",
    "\tlayer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cc9e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR = 1e-4\n",
    "NUM_EPOCHS = 10\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / NUM_EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "H = model.fit_generator(\n",
    "\ttrainGen,\n",
    "\tsteps_per_epoch=totalTrain // BATCH_SIZE,\n",
    "\tvalidation_data=valGen,\n",
    "\tvalidation_steps=totalVal // BATCH_SIZE,\n",
    "\tepochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79278bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the testing generator and then use our trained model to\n",
    "# make predictions on the data\n",
    "MODEL_PATH = os.path.sep.join([\"output\",\"resnet1\"])\n",
    "print(\"[INFO] evaluating network...\")\n",
    "testGen.reset()\n",
    "predIdxs = model.predict_generator(testGen,\n",
    "\tsteps=(totalTest // BATCH_SIZE) + 1)\n",
    "# for each image in the testing set we need to find the index of the\n",
    "# label with corresponding largest predicted probability\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "# show a nicely formatted classification report\n",
    "print(classification_report(testGen.classes, predIdxs,\n",
    "\ttarget_names=testGen.class_indices.keys()))\n",
    "# serialize the model to disk\n",
    "print(\"[INFO] saving model...\")\n",
    "model.save(MODEL_PATH, save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f094c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90667eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the changes to the model to take affect we need to recompile\n",
    "# the model, this time using SGD with a *very* small learning rate\n",
    "print(\"[INFO] re-compiling model...\")\n",
    "opt = SGD(lr=1e-4, momentum=0.9)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "# train the model again, this time fine-tuning *both* the final set\n",
    "# of CONV layers along with our set of FC layers\n",
    "H = model.fit(\n",
    "\tx=trainGen,\n",
    "\tsteps_per_epoch=totalTrain // BATCH_SIZE,\n",
    "\tvalidation_data=valGen,\n",
    "\tvalidation_steps=totalVal // BATCH_SIZE,\n",
    "\tepochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1467c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the testing generator and then use our trained model to\n",
    "# make predictions on the data\n",
    "MODEL_PATH = os.path.sep.join([\"output\",\"vgg1\"])\n",
    "print(\"[INFO] evaluating after fine-tuning network...\")\n",
    "testGen.reset()\n",
    "predIdxs = model.predict(x=testGen,\n",
    "\tsteps=(totalTest // BATCH_SIZE) + 1)\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "print(classification_report(testGen.classes, predIdxs,\n",
    "\ttarget_names=testGen.class_indices.keys()))\n",
    "# plot_training(H, 20, config.UNFROZEN_PLOT_PATH)\n",
    "# serialize the model to disk\n",
    "print(\"[INFO] serializing network...\")\n",
    "model.save(MODEL_PATH, save_format=\"h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
